from typing import Any
import jax
import jax.numpy as jnp
import jax.random as random
from jax.lax import lgamma
from waymax import agents
from waymax import datatypes
from waymax import env as _env


def binom_jax(n, k):
    """Computes the binomial coefficient using lgamma for stability."""
    # Ensure inputs are appropriate for lgamma (e.g., non-negative)
    # Convert to float for lgamma compatibility
    n_float = n + 1.0
    k_float = k + 1.0
    n_minus_k_float = n - k + 1.0
    # Add small epsilon to avoid log(0) issues if needed, though lgamma handles positive integers.
    # Check for k < 0 or k > n if necessary.
    return jnp.exp(lgamma(n_float) - lgamma(k_float) - lgamma(n_minus_k_float))


def get_best_action(
    mean_params: jnp.ndarray,
    cholesky_params: jnp.ndarray,
    state: _env.PlanningAgentSimulatorState,
    rollout_env: _env.PlanningAgentEnvironment,
    N: int,
) -> jnp.ndarray:
    """Computes the best action based on the full multivariate Gaussian parameters and state.

    Args:
        gaussian_params: Parameters of the Gaussian distribution.
        dims: Dimension of the mean vector.
        state: Current state.
        rollout_env: Environment to roll out the actions.
        N: Number of action sequences to sample.

    Returns:
        The best action based on evaluating the trajectories generated by the action sequences generated polynomials sampled from the Gaussian distribution.
    """

    # Get the dimension D from the mean parameters
    D = mean_params.shape[0]

    # Create an empty DxD matrix
    cholesky_matrix = jnp.zeros((D, D))

    # Get the indices for the lower triangle
    tril_indices = jnp.tril_indices(D)

    # Fill the lower triangle with the cholesky parameters
    cholesky_matrix = cholesky_matrix.at[tril_indices].set(cholesky_params)

    # Generate a random sample from the multivariate Gaussian distribution
    random_key = random.key(0)  # TODO base seed on timestep
    samples = random.multivariate_normal(
        random_key, mean_params, cholesky_matrix, shape=(N,)
    )

    num_params_per_poly = D // 2

    # Split the samples into two equal parts for two polynomials
    poly1_samples = samples[:, :num_params_per_poly]
    poly2_samples = samples[:, num_params_per_poly:]

    dt = 0.1  # Timestep duration in seconds
    horizon = 5.0  # Planning horizon in seconds
    num_steps = int(round(horizon / dt))

    degree = num_params_per_poly - 1

    # Transform coefficients using tanh to ensure they are in [-1, 1]
    # This guarantees the Bernstein polynomial values will also be in [-1, 1]
    transformed_poly1_coeffs = jnp.tanh(poly1_samples)
    transformed_poly2_coeffs = jnp.tanh(poly2_samples)

    # Generate normalized time steps [0, 1]
    t_norm = jnp.linspace(0, 1, num_steps)

    # Define Bernstein polynomial evaluation function
    def bernstein_poly_eval_norm(coeffs, t, n):
        """Evaluates Bernstein polynomial with coeffs at normalized time t."""
        i = jnp.arange(n + 1)
        # Handle potential 0^0 cases safely, although JAX might handle them.
        # Using where to ensure basis is 0 if t=1 and i!=n, or t=0 and i!=0.
        t_pow_i = jnp.where(i == 0, 1.0, t**i)
        one_minus_t_pow_n_minus_i = jnp.where(n - i == 0, 1.0, (1 - t) ** (n - i))

        basis = binom_jax(n, i) * t_pow_i * one_minus_t_pow_n_minus_i
        return jnp.sum(coeffs * basis, axis=-1)

    # Vectorize the evaluation function
    # Map over time for a single set of coefficients
    eval_single_poly_over_time = jax.vmap(
        lambda c, t: bernstein_poly_eval_norm(c, t, degree), in_axes=(None, 0)
    )
    # Map over different sets of coefficients (samples)
    eval_all_polys_over_time = jax.vmap(eval_single_poly_over_time, in_axes=(0, None))

    # Evaluate both sets of polynomials for all samples over all timesteps
    poly1_values = eval_all_polys_over_time(
        transformed_poly1_coeffs, t_norm
    )  # Shape: (N, num_steps)
    poly2_values = eval_all_polys_over_time(
        transformed_poly2_coeffs, t_norm
    )  # Shape: (N, num_steps)

    # Stack the results to form action sequences (e.g., [steering, acceleration])
    # Shape: (N, num_steps, 2)
    action_sequences = jnp.stack([poly1_values, poly2_values], axis=-1)

    # Generate N random keys for the rollouts
    rollout_keys = random.split(random_key, N)

    # Define actor init and select_action functions (remain the same)
    init = lambda rng, state: {
        "action_index": 0,
    }

    def select_action(
        params: Any, state: datatypes.SimulatorState, actor_state: Any, rng: jax.Array
    ):
        # Get the current action sequence and index
        action_index = actor_state["action_index"]
        # params now contains a single action_sequence for one rollout
        action_sequence = params["action_sequence"]

        # Get the current action
        # Use lax.dynamic_slice to handle potential out-of-bounds if index exceeds num_steps
        # Although rollout should handle stopping. Clamp index for safety if needed.
        current_action = jax.lax.dynamic_slice_in_dim(
            action_sequence, action_index, 1, axis=0
        )
        # Squeeze the time dimension
        action = jnp.squeeze(current_action, axis=0)

        accel = action[0]
        steer = action[1]

        # Create the action object
        action_obj = datatypes.Action(
            data=jnp.array([accel, steer]),
            valid=jnp.array([True]),
        )

        # Update the action index for the next step
        actor_state["action_index"] += 1

        return agents.WaymaxActorOutput(
            actor_state=actor_state,  # type: ignore
            action=action_obj,  # type: ignore
            is_controlled=state.object_metadata.is_sdc,  # type: ignore
        )

    sequence_actor = agents.actor_core_factory(init, select_action)

    # Prepare actor_params for vmap: a pytree where the 'action_sequence' leaf has the batch dim N
    batched_actor_params = {
        "action_sequence": action_sequences,  # Shape (N, num_steps, 2)
    }

    # Define a function for a single rollout to be vmapped
    def single_rollout(rng_key, actor_params_single):
        return _env.rollout(
            scenario=state,  # Initial state is the same for all rollouts
            actor=sequence_actor,
            env=rollout_env,
            rng=rng_key,
            rollout_num_steps=num_steps,
            actor_params=actor_params_single,
        )

    # Vmap the rollout function over random keys and actor parameters
    # in_axes specifies which arguments to map over:
    # 0 for rng_key (first axis of rollout_keys)
    # 0 for actor_params_single (first axis of the 'action_sequence' leaf in batched_actor_params)
    vmapped_rollout = jax.vmap(single_rollout, in_axes=(0, 0))

    # Execute the vmapped rollouts
    # rollout_outputs will be a pytree structure where each leaf has an added dimension N at the beginning
    rollout_outputs = vmapped_rollout(rollout_keys, batched_actor_params)

    # TODO: Evaluate the rollout_outputs (e.g., compute rewards/costs)
    # and select the best action sequence. For now, returning a dummy action.

    dummy_best_action = jnp.zeros(2)  # Placeholder
    return dummy_best_action  # Replace with actual best action after evaluation


jitted_get_best_action = jax.jit(get_best_action, static_argnames=["N", "rollout_env"])
