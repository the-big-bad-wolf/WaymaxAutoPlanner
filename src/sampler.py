import jax
import jax.numpy as jnp
import jax.random as random
from waymax import env as _env
from jax.lax import lgamma


def binom_jax(n, k):
    """Computes the binomial coefficient using lgamma for stability."""
    # Ensure inputs are appropriate for lgamma (e.g., non-negative)
    # Convert to float for lgamma compatibility
    n_float = n + 1.0
    k_float = k + 1.0
    n_minus_k_float = n - k + 1.0
    # Add small epsilon to avoid log(0) issues if needed, though lgamma handles positive integers.
    # Check for k < 0 or k > n if necessary.
    return jnp.exp(lgamma(n_float) - lgamma(k_float) - lgamma(n_minus_k_float))


def get_best_action(
    mean_params: jnp.ndarray,
    cholesky_params: jnp.ndarray,
    state: _env.PlanningAgentSimulatorState,
    rollout_env: _env.PlanningAgentEnvironment,
    N: int,
) -> jnp.ndarray:
    """Computes the best action based on the full multivariate Gaussian parameters and state.

    Args:
        gaussian_params: Parameters of the Gaussian distribution.
        dims: Dimension of the mean vector.
        state: Current state.
        rollout_env: Environment to roll out the actions.
        N: Number of action sequences to sample.

    Returns:
        The best action based on evaluating the trajectories generated by the action sequences generated polynomials sampled from the Gaussian distribution.
    """

    # Get the dimension D from the mean parameters
    D = mean_params.shape[0]

    # Create an empty DxD matrix
    cholesky_matrix = jnp.zeros((D, D))

    # Get the indices for the lower triangle
    tril_indices = jnp.tril_indices(D)

    # Fill the lower triangle with the cholesky parameters
    cholesky_matrix = cholesky_matrix.at[tril_indices].set(cholesky_params)

    # Generate a random sample from the multivariate Gaussian distribution
    key = random.key(0)  # TODO base seed on timestep
    samples = random.multivariate_normal(key, mean_params, cholesky_matrix, shape=(N,))

    assert (
        D % 2 == 0
    ), "Total number of parameters (D) must be even for two equal-degree polynomials."
    num_params_per_poly = D // 2

    # Split the samples into two equal parts for two polynomials
    poly1_samples = samples[:, :num_params_per_poly]
    poly2_samples = samples[:, num_params_per_poly:]

    dt = 0.1  # Timestep duration in seconds
    horizon = 5.0  # Planning horizon in seconds
    num_steps = int(round(horizon / dt))

    degree = num_params_per_poly - 1

    # Transform coefficients using tanh to ensure they are in [-1, 1]
    # This guarantees the Bernstein polynomial values will also be in [-1, 1]
    transformed_poly1_coeffs = jnp.tanh(poly1_samples)
    transformed_poly2_coeffs = jnp.tanh(poly2_samples)

    # Generate normalized time steps [0, 1]
    t_norm = jnp.linspace(0, 1, num_steps)

    # Define Bernstein polynomial evaluation function
    def bernstein_poly_eval_norm(coeffs, t, n):
        """Evaluates Bernstein polynomial with coeffs at normalized time t."""
        i = jnp.arange(n + 1)
        # Handle potential 0^0 cases safely, although JAX might handle them.
        # Using where to ensure basis is 0 if t=1 and i!=n, or t=0 and i!=0.
        t_pow_i = jnp.where(i == 0, 1.0, t**i)
        one_minus_t_pow_n_minus_i = jnp.where(n - i == 0, 1.0, (1 - t) ** (n - i))

        basis = binom_jax(n, i) * t_pow_i * one_minus_t_pow_n_minus_i
        return jnp.sum(coeffs * basis, axis=-1)

    # Vectorize the evaluation function
    # Map over time for a single set of coefficients
    eval_single_poly_over_time = jax.vmap(
        lambda c, t: bernstein_poly_eval_norm(c, t, degree), in_axes=(None, 0)
    )
    # Map over different sets of coefficients (samples)
    eval_all_polys_over_time = jax.vmap(eval_single_poly_over_time, in_axes=(0, None))

    # Evaluate both sets of polynomials for all samples over all timesteps
    poly1_values = eval_all_polys_over_time(
        transformed_poly1_coeffs, t_norm
    )  # Shape: (N, num_steps)
    poly2_values = eval_all_polys_over_time(
        transformed_poly2_coeffs, t_norm
    )  # Shape: (N, num_steps)

    # Stack the results to form action sequences (e.g., [steering, acceleration])
    # Shape: (N, num_steps, 2)
    action_sequences = jnp.stack([poly1_values, poly2_values], axis=-1)

    # TODO: Evaluate these action sequences using the rollout_env and find the best one
    # This part is left for subsequent steps based on the function's goal.
    # For now, we might return the sequences or proceed with evaluation.
    # The function signature implies returning the *best action*, which likely means
    # evaluating these sequences and selecting the best initial action.
    # Placeholder for trajectory evaluation and selection logic:
    # costs = evaluate_trajectories(action_sequences, state, rollout_env)
    # best_sequence_index = jnp.argmin(costs)
    # best_action = action_sequences[best_sequence_index, 0, :] # Example: return first action of best sequence
    # return best_action # Or potentially the best sequence: action_sequences[best_sequence_index]

    # Returning the generated sequences for now, evaluation logic needs to be added
    # based on how 'best_action' is defined (first action vs full sequence).
    # Assuming the goal is to evaluate these sequences later:
    # (The function name suggests returning a single action, implying evaluation happens here)

    # --- Placeholder for evaluation logic ---
    # This requires rolling out each sequence in the environment.
    # Due to complexity and potential statefulness/randomness in rollout,
    # this part needs careful implementation, possibly using jax.lax.scan or vmap.

    # Example structure (needs actual implementation):
    # def rollout_fn(action_sequence):
    #    final_state, _ = jax.lax.scan(rollout_env.step, state, action_sequence)
    #    cost = calculate_cost(final_state) # Define calculate_cost based on objectives
    #    return cost
    #
    # costs = jax.vmap(rollout_fn)(action_sequences)
    # best_index = jnp.argmin(costs)
    # best_action_sequence = action_sequences[best_index]
    # return best_action_sequence[0] # Return the first action of the best sequence

    # For now, returning the sequences as the function doesn't implement evaluation yet.
    # This might need adjustment based on the full intended logic.
    # If the function MUST return a single best action (shape typically (2,)),
    # the evaluation part is non-trivial and needs the cost function definition.
    # Let's assume for now the function's goal includes evaluation and returns the first action.
    # Since evaluation logic is missing, we'll return a dummy value matching expected output type.
    # Replace this with actual evaluation and selection.
    dummy_best_action = jnp.zeros(2)  # Placeholder
    return dummy_best_action  # Replace with actual best action after evaluation


jitted_get_best_action = jax.jit(get_best_action, static_argnames=["N", "rollout_env"])
