{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DMEnv\n",
    "import dataclasses\n",
    "from typing import Any, Iterator, Tuple, Union\n",
    "\n",
    "import flax.linen as nn\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import mediapy\n",
    "import numpy as np\n",
    "import skrl\n",
    "import skrl.envs.wrappers.jax as skrl_wrappers\n",
    "from jax import jit\n",
    "from jax import numpy as jnp\n",
    "from skrl.agents.jax.ppo import PPO, PPO_DEFAULT_CONFIG\n",
    "from skrl.models.jax import GaussianMixin, Model\n",
    "from skrl.trainers.jax import SequentialTrainer\n",
    "from skrl.utils.spaces.jax import flatten_tensorized_space, tensorize_space\n",
    "from tqdm import tqdm\n",
    "from waymax import agents\n",
    "from waymax import config as _config\n",
    "from waymax import dataloader, datatypes, dynamics\n",
    "from waymax import env as _env\n",
    "from waymax import visualization\n",
    "from waymax.agents.actor_core import ActorState\n",
    "from waymax.env.wrappers.dm_env_wrapper import make_sdc_dm_environment\n",
    "\n",
    "# path = \"gs://waymo_open_dataset_motion_v_1_3_0/uncompressed/tf_example/training/training_tfexample.tfrecord@1000\"\n",
    "path = \"./data/training_tfexample.tfrecord@5\"\n",
    "max_num_objects = 32\n",
    "data_loader_config = dataclasses.replace(\n",
    "    _config.WOD_1_1_0_TRAINING,\n",
    "    path=path,\n",
    "    max_num_objects=max_num_objects,\n",
    "    max_num_rg_points=30000,\n",
    ")\n",
    "data_iter = dataloader.simulator_state_generator(config=data_loader_config)\n",
    "sim_agent_config = _config.SimAgentConfig(\n",
    "    agent_type=_config.SimAgentType.IDM, controlled_objects=_config.ObjectType.NON_SDC\n",
    ")\n",
    "env_config = dataclasses.replace(\n",
    "    _config.EnvironmentConfig(),\n",
    "    max_num_objects=max_num_objects,\n",
    "    sim_agents=[sim_agent_config],\n",
    ")\n",
    "dynamics_model = dynamics.InvertibleBicycleModel(normalize_actions=True)\n",
    "env=make_sdc_dm_environment(dynamics_model,data_loader_config, env_config)\n",
    "actor = agents.create_expert_actor(dynamics_model=dynamics_model)\n",
    "jit_select_action = jax.jit(actor.select_action)\n",
    "jit_select_by_onehot = jax.jit(datatypes.select_by_onehot)\n",
    "\n",
    "\n",
    "# Reset environment with new state\n",
    "timestep=env.reset()\n",
    "rng = jax.random.key(0)\n",
    "actor_state = actor.init(rng, env.simulation_state)\n",
    "\n",
    "\n",
    "# Run the environment\n",
    "@jit\n",
    "def run(timestep: TimeStep, actor_state: ActorState, rng):\n",
    "    states = [timestep.state]\n",
    "    for _ in tqdm(range(80)):\n",
    "        action = jit_select_action({}, env.simulation_state, actor_state, rng).action\n",
    "        action = jit_select_by_onehot(action, timestep.state.object_metadata.is_sdc)\n",
    "        timestep = env.step(action)\n",
    "        states.append(env.simulation_state)\n",
    "    return states\n",
    "    \n",
    "states=run(timestep, actor_state, rng)\n",
    "\n",
    "\n",
    "# Visualize the scenario\n",
    "imgs = []\n",
    "for state in states:\n",
    "    imgs.append(visualization.plot_simulator_state(state, use_log_traj=False))\n",
    "mediapy.show_video(imgs, fps=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;20m[skrl:WARNING] IsaacGymEnvs runs on GPU, but there is no GPU backend for JAX. JAX operations will run on CPU.\u001b[0m\n",
      "\u001b[33;20m[skrl:WARNING] Isaac Lab runs on GPU, but there is no GPU backend for JAX. JAX operations will run on CPU.\u001b[0m\n",
      "\u001b[33;20m[skrl:WARNING] OmniIsaacGymEnvs runs on GPU, but there is no GPU backend for JAX. JAX operations will run on CPU.\u001b[0m\n",
      "\u001b[33;20m[skrl:WARNING] Invalid device specification (cuda:0): Unknown backend cuda. Available backends are ['cpu']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._jax True\n",
      "  0%|          | 15/100000 [00:06<4:39:44,  5.96it/s] "
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "from typing import Any, Dict, Iterator, List, Sequence, Tuple, Union, override\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import gymnasium\n",
    "import skrl\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import mediapy\n",
    "import numpy as np\n",
    "import skrl.envs.wrappers.jax as skrl_wrappers\n",
    "from skrl import config\n",
    "from dm_env import specs\n",
    "from jax import jit\n",
    "from skrl.agents.jax.ppo import PPO, PPO_DEFAULT_CONFIG\n",
    "from skrl.memories.jax import RandomMemory\n",
    "from skrl.models.jax import DeterministicMixin, GaussianMixin, Model\n",
    "from skrl.trainers.jax import SequentialTrainer\n",
    "from tqdm import tqdm\n",
    "from waymax import agents\n",
    "from waymax import config as _config\n",
    "from waymax import dataloader, datatypes, dynamics\n",
    "from waymax import env as _env\n",
    "from waymax import visualization\n",
    "\n",
    "# Set the backend to \"jax\" or \"numpy\"\n",
    "config.jax.backend = \"jax\"\n",
    "\n",
    "# path = \"gs://waymo_open_dataset_motion_v_1_3_0/uncompressed/tf_example/training/training_tfexample.tfrecord@1000\"\n",
    "path = \"./data/training_tfexample.tfrecord@5\"\n",
    "max_num_objects = 32\n",
    "data_loader_config = dataclasses.replace(\n",
    "    _config.WOD_1_1_0_TRAINING,\n",
    "    path=path,\n",
    "    max_num_objects=max_num_objects,\n",
    "    max_num_rg_points=30000,\n",
    ")\n",
    "data_iter = dataloader.simulator_state_generator(config=data_loader_config)\n",
    "sim_agent_config = _config.SimAgentConfig(\n",
    "    agent_type=_config.SimAgentType.IDM, controlled_objects=_config.ObjectType.NON_SDC\n",
    ")\n",
    "env_config = dataclasses.replace(\n",
    "    _config.EnvironmentConfig(),\n",
    "    max_num_objects=max_num_objects,\n",
    "    sim_agents=[sim_agent_config],\n",
    ")\n",
    "dynamics_model = dynamics.InvertibleBicycleModel(normalize_actions=True)\n",
    "env = _env.PlanningAgentEnvironment(\n",
    "    dynamics_model=dynamics_model,\n",
    "    config=env_config,\n",
    "    sim_agent_actors=[agents.create_sim_agents_from_config(sim_agent_config)],\n",
    "    sim_agent_params=[{}],\n",
    ")\n",
    "actor = agents.create_expert_actor(dynamics_model=dynamics_model)\n",
    "\n",
    "\n",
    "class WaymaxWrapper(skrl_wrappers.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: _env.PlanningAgentEnvironment,\n",
    "        scenario_loader: Iterator[datatypes.SimulatorState],\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self._jax = True\n",
    "        self._env = env\n",
    "        self._scenario_loader = scenario_loader\n",
    "        self._jit_step = jit(self._env.step)\n",
    "        self._jit_reset = jit(self._env.reset)\n",
    "        self._jit_observe = jit(self._env.observe)\n",
    "        self._jit_reward = jit(self._env.reward)\n",
    "        self._jit_truncation = jit(self._env.truncation)\n",
    "        self._jit_termination = jit(self._env.termination)\n",
    "\n",
    "    @override\n",
    "    def reset(self) -> Tuple[Union[np.ndarray, jax.Array], Any]:\n",
    "        \"\"\"Reset the environment\n",
    "\n",
    "        :return: Observation, info\n",
    "        :rtype: np.ndarray or jax.Array and any other info\n",
    "        \"\"\"\n",
    "        self._state = self._env.reset(next(self._scenario_loader))\n",
    "        observation = self._jit_observe(self._state)\n",
    "        return observation, {}\n",
    "\n",
    "    @override\n",
    "    def step(self, actions: Union[np.ndarray, jax.Array]) -> Tuple[\n",
    "        Union[np.ndarray, jax.Array],\n",
    "        Union[np.ndarray, jax.Array],\n",
    "        Union[np.ndarray, jax.Array],\n",
    "        Union[np.ndarray, jax.Array],\n",
    "        Any,\n",
    "    ]:\n",
    "        \"\"\"Perform a step in the environment\n",
    "\n",
    "        :param actions: The actions to perform\n",
    "        :type actions: np.ndarray or jax.Array\n",
    "\n",
    "        :return: Observation, reward, terminated, truncated, info\n",
    "        :rtype: tuple of np.ndarray or jax.Array and any other info\n",
    "        \"\"\"\n",
    "        action = datatypes.Action(\n",
    "            data=jnp.array(actions), valid=jnp.array([True])  # type: ignore\n",
    "        )\n",
    "        self._state = self._env.step(self._state, action)\n",
    "        reward = self._env.reward(self._state, action)\n",
    "        reward = jnp.array([reward])  # type: ignore\n",
    "        observation = self._env.observe(self._state)\n",
    "        terminated = self._env.termination(self._state)\n",
    "        truncated = self._env.truncation(self._state)\n",
    "        return observation, reward, terminated, truncated, {}\n",
    "\n",
    "    def state(self) -> Union[np.ndarray, jax.Array]:\n",
    "        \"\"\"Get the environment state\n",
    "\n",
    "        :raises NotImplementedError: Not implemented\n",
    "\n",
    "        :return: State\n",
    "        :rtype: np.ndarray or jax.Array\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def render(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Render the environment\n",
    "\n",
    "        :return: Any value from the wrapped environment\n",
    "        :rtype: any\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close the environment\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def observation_space(self) -> gymnasium.Space:\n",
    "        \"\"\"The observation specs of this environment, without batch dimension.\"\"\"\n",
    "        observation_spec: specs.BoundedArray = self._env.observation_spec()\n",
    "        return gymnasium.spaces.Box(\n",
    "            low=observation_spec.minimum, high=observation_spec.maximum, dtype=observation_spec.dtype  # type: ignore\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def action_space(self) -> gymnasium.Space:\n",
    "        \"\"\"Action space\"\"\"\n",
    "        action_spec: specs.BoundedArray = self._env.action_spec().data  # type: ignore\n",
    "        return gymnasium.spaces.Box(\n",
    "            low=action_spec.minimum, high=action_spec.maximum, dtype=action_spec.dtype  # type: ignore\n",
    "        )\n",
    "\n",
    "\n",
    "env = WaymaxWrapper(env, data_iter)\n",
    "\n",
    "\n",
    "class Policy(GaussianMixin, Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        device=None,\n",
    "        clip_actions=False,\n",
    "        clip_log_std=True,\n",
    "        min_log_std=-20,\n",
    "        max_log_std=2,\n",
    "        reduction=\"sum\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        Model.__init__(self, observation_space, action_space, device, **kwargs)\n",
    "        GaussianMixin.__init__(\n",
    "            self, clip_actions, clip_log_std, min_log_std, max_log_std, reduction\n",
    "        )\n",
    "\n",
    "    @nn.compact  # marks the given module method allowing inlined submodules\n",
    "    def __call__(self, inputs, role):\n",
    "        x = nn.relu(nn.Dense(64)(inputs[\"states\"]))\n",
    "        x = nn.relu(nn.Dense(64)(x))\n",
    "        x = nn.Dense(self.num_actions)(x)\n",
    "        log_std = self.param(\"log_std\", lambda _: jnp.zeros(self.num_actions))\n",
    "        # Pendulum-v1 action_space is -2 to 2\n",
    "        return 2 * nn.tanh(x), log_std, {}\n",
    "\n",
    "\n",
    "class Value(DeterministicMixin, Model):\n",
    "    def __init__(\n",
    "        self, observation_space, action_space, device=None, clip_actions=False, **kwargs\n",
    "    ):\n",
    "        Model.__init__(self, observation_space, action_space, device, **kwargs)\n",
    "        DeterministicMixin.__init__(self, clip_actions)\n",
    "\n",
    "    @nn.compact  # marks the given module method allowing inlined submodules\n",
    "    def __call__(self, inputs, role):\n",
    "        x = nn.relu(nn.Dense(64)(inputs[\"states\"]))\n",
    "        x = nn.relu(nn.Dense(64)(x))\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x, {}\n",
    "\n",
    "\n",
    "# instantiate a memory as rollout buffer (any memory can be used for this)\n",
    "memory = RandomMemory(memory_size=1024, num_envs=env.num_envs)\n",
    "\n",
    "\n",
    "# instantiate the agent's models (function approximators).\n",
    "# PPO requires 2 models, visit its documentation for more details\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/ppo.html#models\n",
    "models = {}\n",
    "models[\"policy\"] = Policy(env.observation_space, env.action_space, clip_actions=True)\n",
    "models[\"value\"] = Value(env.observation_space, env.action_space)\n",
    "\n",
    "# instantiate models' state dict\n",
    "for role, model in models.items():\n",
    "    model.init_state_dict(role)\n",
    "\n",
    "\n",
    "# configure and instantiate the agent (visit its documentation to see all the options)\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/ppo.html#configuration-and-hyperparameters\n",
    "cfg = PPO_DEFAULT_CONFIG.copy()\n",
    "\n",
    "agent = PPO(\n",
    "    models=models,\n",
    "    memory=memory,\n",
    "    cfg=cfg,\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    ")\n",
    "\n",
    "\n",
    "# configure and instantiate the RL trainer\n",
    "cfg_trainer = {\"timesteps\": 100000, \"headless\": True}\n",
    "trainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=[agent])\n",
    "\n",
    "# start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapy\n",
    "from tqdm import tqdm\n",
    "import dataclasses\n",
    "\n",
    "from waymax import config as _config\n",
    "from waymax import dataloader\n",
    "from waymax import datatypes\n",
    "from waymax import visualization\n",
    "\n",
    "max_num_objects = 32\n",
    "config = dataclasses.replace(\n",
    "    _config.WOD_1_1_0_TRAINING,\n",
    "    max_num_objects=max_num_objects,\n",
    "    max_num_rg_points=30000,\n",
    "    path=\"./data/training_tfexample.tfrecord@5\",\n",
    ")\n",
    "data_iter = dataloader.simulator_state_generator(config=config)\n",
    "scenario = next(data_iter)\n",
    "\n",
    "\n",
    "img = visualization.plot_simulator_state(scenario, use_log_traj=True)\n",
    "imgs = [img]\n",
    "state = scenario\n",
    "for _ in range(scenario.remaining_timesteps):\n",
    "  state = datatypes.update_state_by_log(state, num_steps=1)\n",
    "  imgs.append(visualization.plot_simulator_state(state, use_log_traj=True))\n",
    "\n",
    "\n",
    "mediapy.show_video(imgs, fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
